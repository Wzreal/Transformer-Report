import torch
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import pandas as pd
import argparse
from collections import Counter
from typing import Tuple, List, Dict, Optional


def calculate_perplexity(loss: float) -> float:
    """è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼šexp(loss)"""
    return torch.exp(torch.tensor(loss)).item()


def plot_training_curves(
        train_losses: List[float], val_losses: List[float],
        train_perplexities: List[float], val_perplexities: List[float],
        save_path: str = "../results/training_curves.png"
):
    """ç»˜åˆ¶è®­ç»ƒ/éªŒè¯losså’Œå›°æƒ‘åº¦æ›²çº¿"""
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 4))

    # Lossæ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label="Train Loss", color="blue", marker="o")
    plt.plot(epochs, val_losses, label="Val Loss", color="red", marker="s")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.grid(True)

    # å›°æƒ‘åº¦æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_perplexities, label="Train Perplexity", color="blue", marker="o")
    plt.plot(epochs, val_perplexities, label="Val Perplexity", color="red", marker="s")
    plt.xlabel("Epoch")
    plt.ylabel("Perplexity")
    plt.title("Training and Validation Perplexity")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ï¼š{save_path}")


def save_experiment_results(
        hyperparams: Dict, train_loss: float, val_loss: float,
        train_perp: float, val_perp: float, ablation_tag: str = "base",
        save_path: str = "../results/experiment_results.csv"
):
    """ä¿å­˜å®éªŒç»“æœåˆ°CSVè¡¨æ ¼"""
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)

    result_dict = {
        "ablation_tag": ablation_tag,
        "d_model": hyperparams["d_model"],
        "n_layers": hyperparams["n_layers"],
        "n_heads": hyperparams["n_heads"],
        "lr": hyperparams["lr"],
        "batch_size": hyperparams["batch_size"],
        "train_loss": train_loss,
        "val_loss": val_loss,
        "train_perplexity": train_perp,
        "val_perplexity": val_perp
    }

    # è‹¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¡¨å¤´ï¼›å¦åˆ™è¿½åŠ 
    if not Path(save_path).exists():
        df = pd.DataFrame(columns=result_dict.keys())
        df.to_csv(save_path, index=False)

    df = pd.read_csv(save_path)
    df = pd.concat([df, pd.DataFrame([result_dict])], ignore_index=True)
    df.to_csv(save_path, index=False)
    print(f"ğŸ“‹ å®éªŒç»“æœå·²ä¿å­˜è‡³ï¼š{save_path}")


def load_vocab(vocab_path: str) -> Tuple[Dict[str, int], Dict[int, str]]:
    """åŠ è½½è¯æ±‡è¡¨ï¼ˆwordâ†’idx å’Œ idxâ†’wordï¼‰"""
    with open(vocab_path, "r", encoding="utf-8") as f:
        words = f.read().splitlines()
    word2idx = {word: idx for idx, word in enumerate(words)}
    idx2word = {idx: word for idx, word in enumerate(words)}
    return word2idx, idx2word


def tokenize(text: List[str], word2idx: Dict[str, int], max_seq_len: int) -> torch.Tensor:
    """æ–‡æœ¬tokenizeï¼ˆè½¬æ¢ä¸ºtoken IDï¼‰"""
    tokenized = []
    for sent in text:
        tokens = sent.split()[:max_seq_len]  # æˆªæ–­é•¿å¥å­
        # è½¬æ¢ä¸ºIDï¼Œæœªç™»å½•è¯ç”¨<unk>ï¼ˆç´¢å¼•1ï¼‰ï¼Œpaddingç”¨0
        token_ids = [word2idx.get(word, 1) for word in tokens]
        # paddingåˆ°max_seq_len
        if len(token_ids) < max_seq_len:
            token_ids += [0] * (max_seq_len - len(token_ids))
        tokenized.append(token_ids)
    return torch.tensor(tokenized, dtype=torch.long)


def build_vocab(text_paths: List[str], vocab_size: int) -> List[str]:
    """ä»æ–‡æœ¬æ–‡ä»¶æ„å»ºè¯æ±‡è¡¨ï¼ˆå–top Né«˜é¢‘è¯ï¼Œå«ç‰¹æ®Šç¬¦å·ï¼‰"""
    counter = Counter()
    # è¯»å–æ‰€æœ‰æ–‡æœ¬å¹¶ç»Ÿè®¡è¯é¢‘
    for path in text_paths:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                words = line.strip().split()
                counter.update(words)
    # ä¿ç•™top Né«˜é¢‘è¯ï¼ŒåŠ ä¸Šç‰¹æ®Šç¬¦å·ï¼ˆ<pad>:0, <unk>:1, <sos>:2, <eos>:3ï¼‰
    special_tokens = ["<pad>", "<unk>", "<sos>", "<eos>"]
    # ç¡®ä¿è¯æ±‡è¡¨æ€»å¤§å°ä¸ºvocab_sizeï¼ˆç‰¹æ®Šç¬¦å·+é«˜é¢‘è¯ï¼‰
    top_k = vocab_size - len(special_tokens)
    top_words = [word for word, _ in counter.most_common(top_k)]
    vocab = special_tokens + top_words
    return vocab


def main():
    """å‘½ä»¤è¡Œå…¥å£ï¼šå¤„ç† --generate_vocab å‚æ•°ç”Ÿæˆè¯æ±‡è¡¨"""
    parser = argparse.ArgumentParser(description="å·¥å…·å‡½æ•°ï¼šè¯æ±‡è¡¨ç”Ÿæˆã€æŒ‡æ ‡è®¡ç®—ã€å¯è§†åŒ–ç­‰")
    # è¯æ±‡è¡¨ç”Ÿæˆç›¸å…³å‚æ•°
    parser.add_argument("--generate_vocab", action="store_true", help="ç”Ÿæˆè¯æ±‡è¡¨å¼€å…³ï¼ˆå¿…é€‰ï¼‰")
    parser.add_argument("--src_data", type=str, required=False, help="æºè¯­è¨€æ–‡æœ¬è·¯å¾„ï¼ˆå¦‚è‹±è¯­train_en.txtï¼‰")
    parser.add_argument("--tgt_data", type=str, required=False, help="ç›®æ ‡è¯­è¨€æ–‡æœ¬è·¯å¾„ï¼ˆå¦‚å¾·è¯­train_de.txtï¼‰")
    parser.add_argument("--src_vocab", type=str, required=False, help="æºè¯­è¨€è¯æ±‡è¡¨ä¿å­˜è·¯å¾„ï¼ˆå¦‚src_vocab.txtï¼‰")
    parser.add_argument("--tgt_vocab", type=str, required=False, help="ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨ä¿å­˜è·¯å¾„ï¼ˆå¦‚tgt_vocab.txtï¼‰")
    parser.add_argument("--vocab_size", type=int, default=10000, help="è¯æ±‡è¡¨å¤§å°ï¼ˆåŒ…å«4ä¸ªç‰¹æ®Šç¬¦å·ï¼Œé»˜è®¤10000ï¼‰")

    args = parser.parse_args()

    # æ‰§è¡Œè¯æ±‡è¡¨ç”Ÿæˆ
    if args.generate_vocab:
        # æ£€æŸ¥å¿…è¦å‚æ•°æ˜¯å¦é½å…¨
        required_args = [args.src_data, args.tgt_data, args.src_vocab, args.tgt_vocab]
        if not all(required_args):
            raise ValueError("âŒ ç”Ÿæˆè¯æ±‡è¡¨å¿…é¡»æŒ‡å®šä»¥ä¸‹å‚æ•°ï¼š--src_dataã€--tgt_dataã€--src_vocabã€--tgt_vocab")

        # ç”Ÿæˆæºè¯­è¨€è¯æ±‡è¡¨
        print(f"â³ æ­£åœ¨ç”Ÿæˆæºè¯­è¨€è¯æ±‡è¡¨ï¼ˆä¿å­˜è·¯å¾„ï¼š{args.src_vocab}ï¼‰...")
        src_vocab = build_vocab(text_paths=[args.src_data], vocab_size=args.vocab_size)
        with open(args.src_vocab, "w", encoding="utf-8") as f:
            f.write("\n".join(src_vocab))

        # ç”Ÿæˆç›®æ ‡è¯­è¨€è¯æ±‡è¡¨
        print(f"â³ æ­£åœ¨ç”Ÿæˆç›®æ ‡è¯­è¨€è¯æ±‡è¡¨ï¼ˆä¿å­˜è·¯å¾„ï¼š{args.tgt_vocab}ï¼‰...")
        tgt_vocab = build_vocab(text_paths=[args.tgt_data], vocab_size=args.vocab_size)
        with open(args.tgt_vocab, "w", encoding="utf-8") as f:
            f.write("\n".join(tgt_vocab))

        # è¾“å‡ºç»“æœæç¤º
        print(f"\nâœ… è¯æ±‡è¡¨ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“š æºè¯­è¨€è¯æ±‡è¡¨ï¼š{len(src_vocab)} ä¸ªè¯ï¼ˆç‰¹æ®Šç¬¦å·4ä¸ª + é«˜é¢‘è¯ {len(src_vocab) - 4} ä¸ªï¼‰")
        print(f"ğŸ“š ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨ï¼š{len(tgt_vocab)} ä¸ªè¯ï¼ˆç‰¹æ®Šç¬¦å·4ä¸ª + é«˜é¢‘è¯ {len(tgt_vocab) - 4} ä¸ªï¼‰")
        print(f"ğŸ’¾ æºè¯­è¨€è¯æ±‡è¡¨è·¯å¾„ï¼š{args.src_vocab}")
        print(f"ğŸ’¾ ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨è·¯å¾„ï¼š{args.tgt_vocab}")


if __name__ == "__main__":
    main()