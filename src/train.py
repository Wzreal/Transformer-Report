import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import argparse
import os
import math
import csv
import matplotlib.pyplot as plt
from typing import Optional, Tuple, Dict, List

# -------------------------- 1. æ•°æ®é›†ç±»ï¼ˆé€‚é…åŒè¯­å¯¹æ ¼å¼ï¼‰--------------------------
class BilingualTokenDataset(Dataset):
    def __init__(self, data_path):
        super().__init__()
        self.data = torch.load(data_path)  # æ ¼å¼ï¼š[(en_tensor, de_tensor), ...]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]  # (æºè¯­è¨€å¼ é‡, ç›®æ ‡è¯­è¨€å¼ é‡)

# -------------------------- 2. æ ¸å¿ƒå·¥å…·å‡½æ•° --------------------------
def load_vocab(vocab_path: str) -> Dict[str, int]:
    """åŠ è½½è¯æ±‡è¡¨"""
    vocab = {}
    with open(vocab_path, 'r', encoding='utf-8') as f:
        for line in f:
            if '\t' not in line:
                continue
            word, idx = line.strip().split('\t')
            vocab[word] = int(idx)
    return vocab

def generate_masks(src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """ç”ŸæˆPadding Maskå’ŒFuture Mask"""
    batch_size, src_seq_len = src.size()
    batch_size, tgt_seq_len = tgt.size()

    # Padding Mask
    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, src_seq_len)
    tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)  # (B, 1, tgt_seq_len, 1)

    # Future Mask
    future_mask = torch.triu(torch.ones((tgt_seq_len, tgt_seq_len), device=src.device), diagonal=1)
    tgt_mask = tgt_pad_mask & (future_mask == 0)  # åˆå¹¶æ©ç 

    return src_mask, tgt_mask

def plot_training_curves(
    train_losses: List[float],
    val_losses: List[float],
    train_perplexities: List[float],
    val_perplexities: List[float],
    ablation_tag: str,
    save_dir: str
):
    """ç»˜åˆ¶è®­ç»ƒ/éªŒè¯æ›²çº¿ï¼ˆæŸå¤±+å›°æƒ‘åº¦ï¼‰ï¼Œä¿å­˜ä¸ºé«˜æ¸…å›¾ç‰‡ï¼ˆä½œä¸šç›´æ¥æ’å…¥ï¼‰"""
    plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡æ”¯æŒ
    plt.rcParams['axes.unicode_minus'] = False    # è´Ÿå·æ”¯æŒ
    plt.rcParams['figure.dpi'] = 300              # é«˜æ¸…å›¾
    plt.rcParams['savefig.dpi'] = 300

    epochs = range(1, len(train_losses) + 1)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # 1. æŸå¤±æ›²çº¿
    ax1.plot(epochs, train_losses, 'o-', color='#2E86AB', label='è®­ç»ƒæŸå¤±', linewidth=2.5, markersize=6)
    ax1.plot(epochs, val_losses, 's-', color='#A23B72', label='éªŒè¯æŸå¤±', linewidth=2.5, markersize=6)
    ax1.set_xlabel('è®­ç»ƒè½®æ•°ï¼ˆEpochï¼‰', fontsize=12)
    ax1.set_ylabel('æŸå¤±å€¼ï¼ˆLossï¼‰', fontsize=12)
    ax1.set_title(f'{ablation_tag} - è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(epochs)

    # 2. å›°æƒ‘åº¦æ›²çº¿
    ax2.plot(epochs, train_perplexities, 'o-', color='#F18F01', label='è®­ç»ƒå›°æƒ‘åº¦', linewidth=2.5, markersize=6)
    ax2.plot(epochs, val_perplexities, 's-', color='#C73E1D', label='éªŒè¯å›°æƒ‘åº¦', linewidth=2.5, markersize=6)
    ax2.set_xlabel('è®­ç»ƒè½®æ•°ï¼ˆEpochï¼‰', fontsize=12)
    ax2.set_ylabel('å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰', fontsize=12)
    ax2.set_title(f'{ablation_tag} - è®­ç»ƒ/éªŒè¯å›°æƒ‘åº¦æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    ax2.set_xticks(epochs)

    # ä¿å­˜å›¾ç‰‡
    save_path = os.path.join(save_dir, f'training_curves_{ablation_tag}.png')
    plt.tight_layout()
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.close()
    print(f"ğŸ“Š æ›²çº¿å·²ä¿å­˜è‡³ï¼š{save_path}")

def save_hyperparameter_table(hyperparams: Dict[str, any], save_path: str):
    """ç”Ÿæˆä½œä¸šè¦æ±‚çš„è¶…å‚æ•°è¡¨æ ¼ï¼ˆCSVæ ¼å¼ï¼Œå¯ç›´æ¥å¤åˆ¶åˆ°Wordï¼‰"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
    file_exists = os.path.exists(save_path)
    with open(save_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f, delimiter='\t')  # åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œæ–¹ä¾¿Wordç²˜è´´
        if not file_exists:
            # è¡¨å¤´ï¼ˆå¯¹åº”ä½œä¸šè¡¨3çš„åˆ—ï¼‰
            writer.writerow(['æ¨¡å‹æ ‡ç­¾', 'åµŒå…¥ç»´åº¦', 'æ³¨æ„åŠ›å¤´æ•°', 'FFNç»´åº¦', 'å±‚æ•°', 'æ‰¹æ¬¡å¤§å°', 'å­¦ä¹ ç‡', 'ä¼˜åŒ–å™¨', 'å­¦ä¹ ç‡è°ƒåº¦å™¨', 'è®­ç»ƒè½®æ•°'])
        # å†™å…¥å½“å‰æ¨¡å‹çš„è¶…å‚æ•°
        writer.writerow([
            hyperparams['ablation_tag'],
            hyperparams['d_model'],
            hyperparams['n_heads'],
            hyperparams['d_ff'],
            hyperparams['n_layers'],
            hyperparams['batch_size'],
            hyperparams['lr'],
            hyperparams['optimizer'],
            hyperparams['scheduler'],
            hyperparams['epochs']
        ])
    print(f"ğŸ“‹ è¶…å‚æ•°å·²è®°å½•è‡³ï¼š{save_path}")

def save_experiment_results(results: Dict[str, any], save_path: str):
    """ä¿å­˜é‡åŒ–ç»“æœï¼ˆæŸå¤±+å›°æƒ‘åº¦ï¼‰ï¼Œç”¨äºæ¨¡å‹å¯¹æ¯”"""
    file_exists = os.path.exists(save_path)
    with open(save_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f, delimiter='\t')
        if not file_exists:
            writer.writerow(['æ¨¡å‹æ ‡ç­¾', 'æœ€ç»ˆè®­ç»ƒæŸå¤±', 'æœ€ç»ˆéªŒè¯æŸå¤±', 'æœ€ç»ˆè®­ç»ƒå›°æƒ‘åº¦', 'æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦', 'æœ€ä¼˜éªŒè¯æŸå¤±', 'æœ€ä¼˜éªŒè¯å›°æƒ‘åº¦'])
        writer.writerow([
            results['ablation_tag'],
            round(results['final_train_loss'], 4),
            round(results['final_val_loss'], 4),
            round(results['final_train_perp'], 2),
            round(results['final_val_perp'], 2),
            round(results['best_val_loss'], 4),
            round(results['best_val_perp'], 2)
        ])
    print(f"ğŸ“ˆ å®éªŒç»“æœå·²è®°å½•è‡³ï¼š{save_path}")

def generate_translation_samples(
    model: torch.nn.Module,
    test_data: BilingualTokenDataset,
    src_vocab: Dict[str, int],
    tgt_vocab: Dict[str, int],
    device: torch.device,
    num_samples: int = 3
) -> List[Dict[str, str]]:
    model.eval()
    # è¯æ±‡è¡¨åå‘æ˜ å°„ï¼ˆindexâ†’wordï¼‰
    src_idx2word = {idx: word for word, idx in src_vocab.items()}
    tgt_idx2word = {idx: word for word, idx in tgt_vocab.items()}

    samples = []
    with torch.no_grad():
        for i in range(num_samples):
            src_tensor, tgt_true_tensor = test_data[i]
            src_tensor = src_tensor.unsqueeze(0).to(device)  # (1, seq_len)

            # è´ªå¿ƒè§£ç ç”Ÿæˆé¢„æµ‹
            tgt_pred_tensor = torch.tensor([tgt_vocab['<sos>']], device=device).unsqueeze(0)  # åˆå§‹åŒ–<SOS>
            for _ in range(len(tgt_true_tensor)-1):
                if tgt_pred_tensor[0, -1].item() == tgt_vocab['<eos>']:
                    break  # é‡åˆ°<eos>åœæ­¢
                output = model(src_tensor, tgt_pred_tensor)
                next_token = output.argmax(-1)[:, -1].unsqueeze(1)
                tgt_pred_tensor = torch.cat([tgt_pred_tensor, next_token], dim=1)

            # è½¬æ¢ä¸ºæ–‡å­—ï¼ˆè¿‡æ»¤<PAD>ã€<SOS>ã€<EOS>ï¼‰
            def tensor_to_sentence(tensor, idx2word):
                return ' '.join([
                    idx2word[idx.item()] for idx in tensor
                    # ä¿®å¤ï¼šè¿‡æ»¤çš„æ˜¯å•è¯ï¼Œä¸æ˜¯ç´¢å¼•ï¼
                    if idx2word[idx.item()] not in ['<pad>', '<sos>', '<eos>']
                ])

            src_sent = tensor_to_sentence(src_tensor[0], src_idx2word)
            tgt_true_sent = tensor_to_sentence(tgt_true_tensor, tgt_idx2word)
            tgt_pred_sent = tensor_to_sentence(tgt_pred_tensor[0], tgt_idx2word)

            samples.append({
                'source': src_sent,
                'prediction': tgt_pred_sent,
                'ground_truth': tgt_true_sent
            })
    return samples

# -------------------------- 3. Transformeræ ¸å¿ƒç»„ä»¶ --------------------------
class ScaledDotProductAttention(nn.Module):
    def __init__(self, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn_weights = torch.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        output = torch.matmul(attn_weights, V)
        return output, attn_weights

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=128, n_heads=2, dropout=0.1, ablate_multi_head=False):
        super().__init__()
        self.n_heads = 1 if ablate_multi_head else n_heads
        self.d_k = d_model // self.n_heads
        # æ–°å¢ï¼šæ˜¾å¼ç»‘å®š d_model ä¸ºå®ä¾‹å±æ€§ï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
        self.d_model = d_model  # è¿™ä¸€è¡Œå¿…é¡»æ·»åŠ 
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, Q, K, V, mask=None):
        residual = Q
        batch_size = Q.size(0)
        # å¤šå¤´æ‹†åˆ†
        Q = self.w_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        # æ³¨æ„åŠ›è®¡ç®—
        attn_output, _ = self.attention(Q, K, V, mask)
        # å¤šå¤´åˆå¹¶ï¼ˆä½¿ç”¨ self.d_modelï¼Œå·²é€šè¿‡ __init__ å®šä¹‰ï¼‰
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        attn_output = self.dropout(self.w_o(attn_output))
        return self.layer_norm(residual + attn_output), None

class PositionWiseFFN(nn.Module):
    def __init__(self, d_model=128, d_ff=512, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        residual = x
        output = self.w_2(self.dropout(self.relu(self.w_1(x))))
        return self.layer_norm(residual + output)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model=128, max_seq_len=10, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        # ç”Ÿæˆä½ç½®ç¼–ç 
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term) if d_model % 2 == 0 else torch.cos(position * div_term[:-1])
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class EncoderLayer(nn.Module):
    def __init__(self, d_model=128, n_heads=2, d_ff=512, dropout=0.1, ablate_multi_head=False):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout, ablate_multi_head)
        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)

    def forward(self, x, mask=None):
        x, _ = self.self_attn(x, x, x, mask)
        x = self.ffn(x)
        return x

class DecoderLayer(nn.Module):
    def __init__(self, d_model=128, n_heads=2, d_ff=512, dropout=0.1, ablate_multi_head=False):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout, ablate_multi_head)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout, ablate_multi_head)
        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)

    def forward(self, x, enc_output, tgt_mask=None, src_mask=None):
        x, _ = self.self_attn(x, x, x, tgt_mask)
        x, _ = self.cross_attn(x, enc_output, enc_output, src_mask)
        x = self.ffn(x)
        return x

class Encoder(nn.Module):
    def __init__(self, src_vocab_size, d_model=128, n_layers=2, n_heads=2, d_ff=512, max_seq_len=10, dropout=0.1, ablate_multi_head=False):
        super().__init__()
        self.embedding = nn.Embedding(src_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)
        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout, ablate_multi_head) for _ in range(n_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        x = self.embedding(src)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, src_mask)
        return x

class Decoder(nn.Module):
    def __init__(self, tgt_vocab_size, d_model=128, n_layers=2, n_heads=2, d_ff=512, max_seq_len=10, dropout=0.1, ablate_pos_encoding=False, ablate_multi_head=False):
        super().__init__()
        self.ablate_pos_encoding = ablate_pos_encoding
        self.embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout) if not ablate_pos_encoding else None
        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout, ablate_multi_head) for _ in range(n_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt, enc_output, tgt_mask=None, src_mask=None):
        x = self.embedding(tgt)
        if not self.ablate_pos_encoding and self.pos_encoding is not None:
            x = self.pos_encoding(x)
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, enc_output, tgt_mask, src_mask)
        return x

class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=128, n_layers=2, n_heads=2, d_ff=512, max_seq_len=10, dropout=0.1, ablate_pos_encoding=False, ablate_multi_head=False):
        super().__init__()
        self.encoder = Encoder(src_vocab_size, d_model, n_layers, n_heads, d_ff, max_seq_len, dropout, ablate_multi_head)
        self.decoder = Decoder(tgt_vocab_size, d_model, n_layers, n_heads, d_ff, max_seq_len, dropout, ablate_pos_encoding, ablate_multi_head)
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        if src_mask is None or tgt_mask is None:
            src_mask, tgt_mask = generate_masks(src, tgt)
        enc_output = self.encoder(src, src_mask)
        dec_output = self.decoder(tgt, enc_output, tgt_mask, src_mask)
        output = self.fc_out(dec_output)
        return output

# -------------------------- 4. è®­ç»ƒä¸»å‡½æ•°ï¼ˆæ ¸å¿ƒï¼‰--------------------------
def train_transformer(args):
    # 1. è®¾å¤‡åˆå§‹åŒ–
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # 2. æ•°æ®åŠ è½½
    src_vocab = load_vocab(args.src_vocab)
    tgt_vocab = load_vocab(args.tgt_vocab)
    train_dataset = BilingualTokenDataset(args.train_data)
    val_dataset = BilingualTokenDataset(args.val_data)

    train_loader = DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=True if device.type == "cuda" else False
    )
    val_loader = DataLoader(
        val_dataset, batch_size=args.batch_size, shuffle=False, pin_memory=True if device.type == "cuda" else False
    )

    print(f"ğŸ“š æ•°æ®åŠ è½½å®Œæˆï¼š")
    print(f"   - æºè¯­è¨€è¯æ±‡è¡¨ï¼š{len(src_vocab)}è¯ | ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨ï¼š{len(tgt_vocab)}è¯")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_dataset)}æ ·æœ¬ | éªŒè¯é›†ï¼š{len(val_dataset)}æ ·æœ¬")
    print(f"   - è®­ç»ƒæ‰¹æ¬¡ï¼š{len(train_loader)} | éªŒè¯æ‰¹æ¬¡ï¼š{len(val_loader)}")

    # 3. æ¨¡å‹åˆå§‹åŒ–
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        d_ff=args.d_ff,
        max_seq_len=args.max_seq_len,
        dropout=args.dropout,
        ablate_pos_encoding=args.ablate_pos_encoding,
        ablate_multi_head=args.ablate_multi_head
    ).to(device)

    # 4. è®­ç»ƒé…ç½®
    criterion = nn.CrossEntropyLoss(ignore_index=src_vocab["<pad>"])  # å¿½ç•¥<PAD>
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)

    # 5. è®­ç»ƒè®°å½•åˆå§‹åŒ–
    train_losses = []
    val_losses = []
    train_perplexities = []
    val_perplexities = []
    best_val_loss = float('inf')
    os.makedirs(args.model_save_dir, exist_ok=True)

    # 6. è¶…å‚æ•°è®°å½•ï¼ˆä½œä¸šè¡¨æ ¼ç”¨ï¼‰
    hyperparams = {
        'ablation_tag': args.ablation_tag,
        'd_model': args.d_model,
        'n_heads': args.n_heads,
        'd_ff': args.d_ff,
        'n_layers': args.n_layers,
        'batch_size': args.batch_size,
        'lr': args.lr,
        'optimizer': 'Adam',
        'scheduler': 'ReduceLROnPlateau',
        'epochs': args.epochs
    }
    save_hyperparameter_table(hyperparams, "../results/hyperparameter_table.tsv")

    # 7. è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆ{args.ablation_tag}ï¼‰ï¼šå…±{args.epochs}ä¸ªEpoch")
    for epoch in range(args.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_total_loss = 0.0
        for batch_idx, (src, tgt) in enumerate(train_loader):
            src, tgt = src.to(device), tgt.to(device)
            optimizer.zero_grad()
            output = model(src, tgt[:, :-1])  # è¾“å…¥å»æ‰<eos>
            loss = criterion(output.reshape(-1, len(tgt_vocab)), tgt[:, 1:].reshape(-1))  # æ ‡ç­¾å»æ‰<sos>
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)
            optimizer.step()
            train_total_loss += loss.item()

            # æ‰¹æ¬¡æ—¥å¿—
            if (batch_idx + 1) % 5 == 0:
                print(f"   Epoch {epoch+1}/{args.epochs} | Batch {batch_idx+1}/{len(train_loader)} | Train Loss: {loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_total_loss = 0.0
        with torch.no_grad():
            for src, tgt in val_loader:
                src, tgt = src.to(device), tgt.to(device)
                output = model(src, tgt[:, :-1])
                loss = criterion(output.reshape(-1, len(tgt_vocab)), tgt[:, 1:].reshape(-1))
                val_total_loss += loss.item()

        # è®¡ç®—æŒ‡æ ‡
        train_avg_loss = train_total_loss / len(train_loader)
        val_avg_loss = val_total_loss / len(val_loader)
        train_perp = torch.exp(torch.tensor(train_avg_loss, device=device)).item()
        val_perp = torch.exp(torch.tensor(val_avg_loss, device=device)).item()

        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_avg_loss)
        val_losses.append(val_avg_loss)
        train_perplexities.append(train_perp)
        val_perplexities.append(val_perp)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_avg_loss)

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if val_avg_loss < best_val_loss:
            best_val_loss = val_avg_loss
            model_path = os.path.join(args.model_save_dir, f"best_model_{args.ablation_tag}.pth")
            torch.save(model.state_dict(), model_path)
            print(f"ğŸ“¥ ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼š{model_path}ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")

        # Epochæ—¥å¿—
        print(f"\n==================================================")
        print(f"Epoch {epoch+1}/{args.epochs} | {args.ablation_tag}")
        print(f"è®­ç»ƒæŸå¤±ï¼š{train_avg_loss:.4f} | è®­ç»ƒå›°æƒ‘åº¦ï¼š{train_perp:.2f}")
        print(f"éªŒè¯æŸå¤±ï¼š{val_avg_loss:.4f} | éªŒè¯å›°æƒ‘åº¦ï¼š{val_perp:.2f}")
        print(f"å½“å‰å­¦ä¹ ç‡ï¼š{optimizer.param_groups[0]['lr']:.6f}")
        print(f"==================================================\n")

    # 8. è®­ç»ƒåå¤„ç†ï¼ˆä½œä¸šæ ¸å¿ƒè¦æ±‚ï¼‰
    # 8.1 ç»˜åˆ¶å¹¶ä¿å­˜æ›²çº¿
    plot_training_curves(
        train_losses=train_losses,
        val_losses=val_losses,
        train_perplexities=train_perplexities,
        val_perplexities=val_perplexities,
        ablation_tag=args.ablation_tag,
        save_dir=args.model_save_dir
    )

    # 8.2 ä¿å­˜å®éªŒç»“æœï¼ˆé‡åŒ–å¯¹æ¯”ç”¨ï¼‰
    experiment_results = {
        'ablation_tag': args.ablation_tag,
        'final_train_loss': train_avg_loss,
        'final_val_loss': val_avg_loss,
        'final_train_perp': train_perp,
        'final_val_perp': val_perp,
        'best_val_loss': best_val_loss,
        'best_val_perp': torch.exp(torch.tensor(best_val_loss)).item()
    }
    save_experiment_results(experiment_results, "../results/experiment_results.tsv")

    # 8.3 ç”Ÿæˆç¿»è¯‘ç¤ºä¾‹ï¼ˆå®šæ€§åˆ†æç”¨ï¼‰
    print(f"\nğŸ“ ç”Ÿæˆç¿»è¯‘ç¤ºä¾‹ï¼ˆ{args.ablation_tag}ï¼‰ï¼š")
    translation_samples = generate_translation_samples(model, val_dataset, src_vocab, tgt_vocab, device, num_samples=3)
    for i, sample in enumerate(translation_samples, 1):
        print(f"\nç¤ºä¾‹ {i}ï¼š")
        print(f"åŸæ–‡ï¼ˆè‹±è¯­ï¼‰ï¼š{sample['source']}")
        print(f"é¢„æµ‹ï¼ˆå¾·è¯­ï¼‰ï¼š{sample['prediction']}")
        print(f"çœŸå®ï¼ˆå¾·è¯­ï¼‰ï¼š{sample['ground_truth']}")

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ ../results/")

# -------------------------- 5. å‘½ä»¤è¡Œå‚æ•°è§£æ --------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="Transformerè®­ç»ƒè„šæœ¬ï¼ˆæ»¡è¶³ä½œä¸šè¦æ±‚ï¼šæ›²çº¿+è¡¨æ ¼+ç¿»è¯‘ç¤ºä¾‹ï¼‰")
    # æ•°æ®è·¯å¾„
    parser.add_argument('--train_data', required=True, help="è®­ç»ƒé›†è·¯å¾„ï¼ˆ.ptï¼‰")
    parser.add_argument('--val_data', required=True, help="éªŒè¯é›†è·¯å¾„ï¼ˆ.ptï¼‰")
    parser.add_argument('--src_vocab', required=True, help="æºè¯­è¨€è¯æ±‡è¡¨ï¼ˆ.txtï¼‰")
    parser.add_argument('--tgt_vocab', required=True, help="ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨ï¼ˆ.txtï¼‰")
    # æ¨¡å‹å‚æ•°ï¼ˆåŒ¹é…ä½œä¸šè¡¨3ï¼‰
    parser.add_argument('--d_model', type=int, default=128, help="åµŒå…¥ç»´åº¦ï¼ˆä½œä¸šè¦æ±‚128ï¼‰")
    parser.add_argument('--n_layers', type=int, default=2, help="å±‚æ•°ï¼ˆä½œä¸šè¦æ±‚2ï¼‰")
    parser.add_argument('--n_heads', type=int, default=4, help="æ³¨æ„åŠ›å¤´æ•°ï¼ˆä½œä¸šè¦æ±‚4ï¼‰")
    parser.add_argument('--d_ff', type=int, default=512, help="FFNç»´åº¦ï¼ˆä½œä¸šè¦æ±‚512ï¼‰")
    parser.add_argument('--max_seq_len', type=int, default=10, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument('--dropout', type=float, default=0.1, help="Dropoutç‡")
    # è®­ç»ƒå‚æ•°ï¼ˆåŒ¹é…ä½œä¸šè¦æ±‚ï¼‰
    parser.add_argument('--batch_size', type=int, default=32, help="æ‰¹æ¬¡å¤§å°ï¼ˆä½œä¸šè¦æ±‚32ï¼‰")
    parser.add_argument('--lr', type=float, default=3e-4, help="å­¦ä¹ ç‡ï¼ˆä½œä¸šè¦æ±‚3e-4ï¼‰")
    parser.add_argument('--epochs', type=int, default=5, help="è®­ç»ƒè½®æ•°")
    parser.add_argument('--clip', type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    # å®éªŒå‚æ•°
    parser.add_argument('--model_save_dir', required=True, help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument('--ablation_tag', required=True, help="æ¨¡å‹æ ‡ç­¾ï¼ˆå¦‚baseã€no_posã€no_multiï¼‰")
    parser.add_argument('--ablate_pos_encoding', action='store_true', help="æ¶ˆèä½ç½®ç¼–ç ï¼ˆä½œä¸šè¦æ±‚ï¼‰")
    parser.add_argument('--ablate_multi_head', action='store_true', help="æ¶ˆèå¤šå¤´æ³¨æ„åŠ›ï¼ˆä½œä¸šè¦æ±‚ï¼‰")
    return parser.parse_args()

# -------------------------- 6. ä¸»å‡½æ•°å…¥å£ --------------------------
if __name__ == "__main__":
    args = parse_args()
    train_transformer(args)