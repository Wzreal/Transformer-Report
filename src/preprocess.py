import re
import random
import torch
from pathlib import Path
from typing import Tuple, Optional, Union
import argparse

# è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰
random.seed(42)
torch.manual_seed(42)


class TEDDatasetPreprocessor:
    def __init__(self,
                 src_lang: str = "en",  # æºè¯­è¨€ï¼ˆen=è‹±è¯­ï¼Œde=å¾·è¯­ï¼‰
                 tgt_lang: Optional[str] = None,  # ç›®æ ‡è¯­è¨€ï¼ˆåŒè¯­é…å¯¹æ—¶è®¾ç½®ï¼Œå¦‚de=å¾·è¯­â†’è‹±è¯­ç¿»è¯‘ï¼‰
                 min_sent_len: int = 5,  # è¿‡æ»¤çŸ­å¥å­ï¼ˆå°‘äº5ä¸ªè¯ï¼‰
                 max_sent_len: int = 128,  # æˆªæ–­é•¿å¥å­ï¼ˆæœ€å¤š128ä¸ªè¯ï¼‰
                 train_ratio: float = 0.8,  # è®­ç»ƒé›†æ¯”ä¾‹
                 val_ratio: float = 0.1):  # éªŒè¯é›†æ¯”ä¾‹ï¼ˆæµ‹è¯•é›†=1-è®­ç»ƒ-éªŒè¯ï¼‰
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.min_len = min_sent_len
        self.max_len = max_sent_len
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = 1 - train_ratio - val_ratio

    def clean_text(self, text: str) -> str:
        """æ¸…æ´—å•å¥æ–‡æœ¬ï¼šå‰”é™¤HTMLæ ‡ç­¾ã€æ³¨é‡Šã€ç‰¹æ®Šç¬¦å·ï¼Œæ ‡å‡†åŒ–æ ¼å¼"""
        # 1. å‰”é™¤HTMLæ ‡ç­¾ï¼ˆ<doc>ã€<url>ç­‰ï¼‰å’ŒCDATAæ³¨é‡Š
        text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤æ‰€æœ‰<>åŒ…è£¹çš„æ ‡ç­¾
        text = re.sub(r'<!\[CDATA\[|\]\]>', '', text)  # ç§»é™¤CDATAæ³¨é‡Š
        # 2. å‰”é™¤ç‰¹æ®Šç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r'[^\w\s\.,!\?;\-]', '', text)  # ä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼å’Œå¸¸è§æ ‡ç‚¹
        text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºä¸€ä¸ªï¼Œå»é™¤é¦–å°¾ç©ºæ ¼
        # 3. å°å†™åŒ–ï¼ˆå¯é€‰ï¼Œæ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼Œè¯­è¨€å»ºæ¨¡å»ºè®®ä¿ç•™å¤§å°å†™ï¼‰
        # text = text.lower()
        return text

    def split_sentences(self, text: str, lang: str) -> list[str]:
        """æŒ‰è¯­è¨€æ‹†åˆ†å¥å­ï¼ˆå¤„ç†è‹±è¯­/å¾·è¯­æ ‡ç‚¹å·®å¼‚ï¼‰"""
        if lang == "en":
            # è‹±è¯­å¥å­ç»“æŸæ ‡ç‚¹ï¼š. ! ? ;
            sentence_endings = re.compile(r'(?<=[.!?;])\s+')
        elif lang == "de":
            # å¾·è¯­å¥å­ç»“æŸæ ‡ç‚¹ï¼š. ! ? ; ï¼ˆæ³¨æ„å¾·è¯­æ ‡ç‚¹åç©ºæ ¼è¦æ±‚ï¼Œè¿™é‡Œç»Ÿä¸€æ‹†åˆ†ï¼‰
            sentence_endings = re.compile(r'(?<=[.!?;])\s+')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€ï¼š{lang}ï¼ˆä»…æ”¯æŒen/deï¼‰")

        sentences = sentence_endings.split(text)
        # è¿‡æ»¤ç©ºå¥å­å’Œè¿‡çŸ­/è¿‡é•¿å¥å­
        filtered = []
        for sent in sentences:
            sent = self.clean_text(sent)
            word_count = len(sent.split())
            if self.min_len <= word_count <= self.max_len:
                filtered.append(sent)
        return filtered

    def load_single_language(self, file_path: Union[str, Path]) -> list[str]:
        """åŠ è½½å•è¯­è¨€TEDæ–‡ä»¶ï¼ˆç”¨äºEncoder-onlyä»»åŠ¡ï¼šè¯­è¨€å»ºæ¨¡/æ–‡æœ¬åˆ†ç±»ï¼‰"""
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")

        # è¯»å–æ–‡ä»¶æ‰€æœ‰å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()

        # æ‹†åˆ†å¥å­å¹¶æ¸…æ´—
        sentences = self.split_sentences(raw_text, self.src_lang)
        print(f"âœ… åŠ è½½{self.src_lang}æ–‡ä»¶å®Œæˆï¼šå…±{len(sentences)}ä¸ªæœ‰æ•ˆå¥å­")
        return sentences

    def load_bilingual_pair(self, src_file: Union[str, Path], tgt_file: Union[str, Path]) -> list[Tuple[str, str]]:
        """åŠ è½½åŒè¯­é…å¯¹æ–‡ä»¶ï¼ˆç”¨äºEncoder-Decoderä»»åŠ¡ï¼šæœºå™¨ç¿»è¯‘ï¼‰"""
        if not self.tgt_lang:
            raise ValueError("åŒè¯­æ¨¡å¼éœ€è®¾ç½®tgt_langï¼ˆå¦‚--tgt_lang deï¼‰")

        # åŠ è½½æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å¥å­
        src_sentences = self.load_single_language(src_file)
        tgt_sentences = self.load_single_language(tgt_file)

        # å¯¹é½å¥å­ï¼ˆä»…ä¿ç•™é•¿åº¦ä¸€è‡´çš„é…å¯¹ï¼Œé¿å…é”™ä½ï¼‰
        min_len = min(len(src_sentences), len(tgt_sentences))
        paired_sentences = list(zip(src_sentences[:min_len], tgt_sentences[:min_len]))
        print(f"âœ… åŒè¯­é…å¯¹å®Œæˆï¼šå…±{len(paired_sentences)}ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹")
        return paired_sentences

    def split_dataset(self, data: Union[list[str], list[Tuple[str, str]]]) -> Tuple[list, list, list]:
        """åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼ˆæŒ‰æ¯”ä¾‹éšæœºæ‹†åˆ†ï¼‰"""
        random.shuffle(data)  # éšæœºæ‰“ä¹±
        total = len(data)
        train_size = int(total * self.train_ratio)
        val_size = int(total * self.val_ratio)

        train = data[:train_size]
        val = data[train_size:train_size + val_size]
        test = data[train_size + val_size:]

        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼š")
        print(f" - è®­ç»ƒé›†ï¼š{len(train)} æ ·æœ¬")
        print(f" - éªŒè¯é›†ï¼š{len(val)} æ ·æœ¬")
        print(f" - æµ‹è¯•é›†ï¼š{len(test)} æ ·æœ¬")
        return train, val, test

    def save_text_format(self, data: Union[list[str], list[Tuple[str, str]]], save_path: Path, split_name: str):
        """ä¿å­˜ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼ˆä¸€è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œä¾¿äºæŸ¥çœ‹å’Œåç»­å¤„ç†ï¼‰"""
        save_path.mkdir(parents=True, exist_ok=True)

        if self.tgt_lang:
            # åŒè¯­æ¨¡å¼ï¼šä¿å­˜ä¸ºsrc-tgté…å¯¹æ–‡ä»¶ï¼ˆæ¯è¡Œæ ¼å¼ï¼šæºè¯­è¨€å¥å­\tç›®æ ‡è¯­è¨€å¥å­ï¼‰
            file_path = save_path / f"{split_name}_{self.src_lang}-{self.tgt_lang}.txt"
            with open(file_path, 'w', encoding='utf-8') as f:
                for src_sent, tgt_sent in data:
                    f.write(f"{src_sent}\t{tgt_sent}\n")
        else:
            # å•è¯­è¨€æ¨¡å¼ï¼šä¿å­˜ä¸ºå•æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªå¥å­ï¼‰
            file_path = save_path / f"{split_name}_{self.src_lang}.txt"
            with open(file_path, 'w', encoding='utf-8') as f:
                for sent in data:
                    f.write(f"{sent}\n")
        print(f"ğŸ’¾ å·²ä¿å­˜{split_name}é›†æ–‡æœ¬æ–‡ä»¶ï¼š{file_path}")

    def save_torch_format(self, data: Union[list[str], list[Tuple[str, str]]], save_path: Path, split_name: str):
        """ä¿å­˜ä¸ºPyTorchå¼ é‡æ ¼å¼ï¼ˆä¾¿äºç›´æ¥åŠ è½½è®­ç»ƒï¼‰"""
        save_path.mkdir(parents=True, exist_ok=True)

        if self.tgt_lang:
            # åŒè¯­æ¨¡å¼ï¼šä¿å­˜ä¸º(src_tensor, tgt_tensor)ï¼ˆè¿™é‡Œå…ˆä¿å­˜å¥å­ç´¢å¼•ï¼Œåç»­ç»“åˆtokenizerï¼‰
            src_sents = [src for src, tgt in data]
            tgt_sents = [tgt for src, tgt in data]
            torch.save((src_sents, tgt_sents), save_path / f"{split_name}_{self.src_lang}-{self.tgt_lang}.pt")
        else:
            # å•è¯­è¨€æ¨¡å¼ï¼šä¿å­˜ä¸ºå¥å­åˆ—è¡¨å¼ é‡
            torch.save(data, save_path / f"{split_name}_{self.src_lang}.pt")
        print(
            f"ğŸ’¾ å·²ä¿å­˜{split_name}é›†PyTorchæ–‡ä»¶ï¼š{save_path / f'{split_name}_{self.src_lang}{"-" + self.tgt_lang if self.tgt_lang else ""}.pt'}")

    def run(self,
            src_file: str,
            tgt_file: Optional[str] = None,
            output_dir: str = "../data/processed"):
        """æ‰§è¡Œå®Œæ•´é¢„å¤„ç†æµç¨‹ï¼šåŠ è½½â†’æ¸…æ´—â†’æ‹†åˆ†â†’ä¿å­˜"""
        output_dir = Path(output_dir)
        print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†ï¼ˆæºè¯­è¨€ï¼š{self.src_lang}ï¼Œç›®æ ‡è¯­è¨€ï¼š{self.tgt_lang or 'æ— '}ï¼‰")

        # 1. åŠ è½½æ•°æ®
        if tgt_file:
            data = self.load_bilingual_pair(src_file, tgt_file)
        else:
            data = self.load_single_language(src_file)

        # 2. åˆ’åˆ†æ•°æ®é›†
        train_data, val_data, test_data = self.split_dataset(data)

        # 3. ä¿å­˜æ–‡ä»¶ï¼ˆåŒæ—¶ä¿å­˜æ–‡æœ¬æ ¼å¼å’ŒPyTorchæ ¼å¼ï¼Œé€‚é…ä¸åŒè®­ç»ƒéœ€æ±‚ï¼‰
        self.save_text_format(train_data, output_dir, "train")
        self.save_text_format(val_data, output_dir, "validation")
        self.save_text_format(test_data, output_dir, "test")

        self.save_torch_format(train_data, output_dir, "train")
        self.save_torch_format(val_data, output_dir, "validation")
        self.save_torch_format(test_data, output_dir, "test")

        print(f"ğŸ‰ é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼ç»“æœä¿å­˜åœ¨ï¼š{output_dir}")


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆæ–¹ä¾¿ä½œä¸šä¸­çµæ´»é…ç½®ï¼‰
    parser = argparse.ArgumentParser(description="TEDæ•°æ®é›†é¢„å¤„ç†ï¼ˆæ”¯æŒå•è¯­è¨€/åŒè¯­ï¼‰")
    parser.add_argument("--src_file", required=True, help="æºè¯­è¨€æ–‡ä»¶è·¯å¾„ï¼ˆè‹±è¯­/å¾·è¯­TEDæ–‡ä»¶ï¼‰")
    parser.add_argument("--tgt_file", default=None, help="ç›®æ ‡è¯­è¨€æ–‡ä»¶è·¯å¾„ï¼ˆåŒè¯­é…å¯¹æ—¶ä½¿ç”¨ï¼Œå¦‚ç¿»è¯‘ä»»åŠ¡ï¼‰")
    parser.add_argument("--src_lang", default="en", choices=["en", "de"], help="æºè¯­è¨€ï¼ˆen=è‹±è¯­ï¼Œde=å¾·è¯­ï¼‰")
    parser.add_argument("--tgt_lang", default=None, choices=["en", "de"], help="ç›®æ ‡è¯­è¨€ï¼ˆåŒè¯­æ—¶è®¾ç½®ï¼Œå¦‚de=å¾·è¯­â†’è‹±è¯­ï¼‰")
    parser.add_argument("--output_dir", default="../data/processed", help="å¤„ç†åæ•°æ®é›†ä¿å­˜ç›®å½•")
    parser.add_argument("--min_sent_len", type=int, default=5, help="æœ€å°å¥å­é•¿åº¦ï¼ˆè¯æ•°ï¼‰")
    parser.add_argument("--max_sent_len", type=int, default=128, help="æœ€å¤§å¥å­é•¿åº¦ï¼ˆè¯æ•°ï¼‰")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹")

    args = parser.parse_args()

    # åˆå§‹åŒ–é¢„å¤„ç†å·¥å…·å¹¶è¿è¡Œ
    preprocessor = TEDDatasetPreprocessor(
        src_lang=args.src_lang,
        tgt_lang=args.tgt_lang,
        min_sent_len=args.min_sent_len,
        max_sent_len=args.max_sent_len,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio
    )

    preprocessor.run(
        src_file=args.src_file,
        tgt_file=args.tgt_file,
        output_dir=args.output_dir
    )